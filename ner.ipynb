{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNkJ9ntCb4FOl0kScPW5cXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs601r-dl/blob/master/ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoyRxyga7yC",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition on Handwritten Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mNs_BtsaykG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2363f371-334d-460b-f1ed-21825371358c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9aDvlkQNOjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"drive/My Drive/datasets/esposalles.zip\" \"/content\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4xL9fP7NtkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q esposalles.zip\n",
        "!rm esposalles.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO8JWMj_gKi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EsposallesDataset(Dataset):\n",
        "  def __init__(self, label='category', img_size=128):\n",
        "    if not os.path.exists('/content/labels.csv'):\n",
        "      raise Exception('Esposalles dataset does not exist in /content/labels.csv')\n",
        "\n",
        "    self.img_size = img_size\n",
        "    self.label = label\n",
        "    self.path = '/content/Images/'\n",
        "    self.labels_df = pd.read_csv('/content/labels.csv', sep='\\t', header=None, names=['word', 'category', 'person', 'transcription', 'page'])\n",
        "\n",
        "    unique_labels = self.labels_df[label].drop_duplicates().values\n",
        "\n",
        "    self.num_to_label = dict()\n",
        "    self.label_to_num = dict()\n",
        "\n",
        "\n",
        "    for index, label in zip(range(len(unique_labels)), unique_labels):\n",
        "      self.num_to_label[index] = label\n",
        "      self.label_to_num[label] = index\n",
        "\n",
        "  def dicts(self):\n",
        "    return self.num_to_label, self.label_to_num\n",
        "  \n",
        "  def num_labels(self):\n",
        "    return len(self.num_to_label)\n",
        "\n",
        "  def resize(self, img):\n",
        "    old_size = img.size\n",
        "    ratio = float(self.img_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "\n",
        "    img = img.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "    new_img = Image.new(\"RGB\", (self.img_size, self.img_size))\n",
        "    new_img.paste(img, ((self.img_size-new_size[0])//2,\n",
        "                        (self.img_size-new_size[1])//2))\n",
        "    \n",
        "    return new_img\n",
        "\n",
        "  def df(self):\n",
        "    return self.labels_df\n",
        "\n",
        "  def open_image(self, path):\n",
        "    img = Image.open(path + '.png')\n",
        "    img = self.resize(img)\n",
        "    x = transforms.functional.to_tensor(img)\n",
        "    # x = x.view(-1, self.img_size ** 2) # Shape the image tensor in a way that can be consumable by the GRU\n",
        "\n",
        "    return x\n",
        "\n",
        "  def __getitem__(self, index):    \n",
        "    pages = self.labels_df[self.labels_df['page'] == index]\n",
        "\n",
        "    imgs = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in pages.iterrows():\n",
        "      img = self.open_image(self.path + row['word'])\n",
        "      label_text = row[self.label]\n",
        "      label_num = self.label_to_num[label_text]\n",
        "\n",
        "      imgs.append(img)\n",
        "      labels.append(label_num)\n",
        "\n",
        "    return imgs, labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels_df['page'].drop_duplicates()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vOsyMHSctcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channel_size, input_size, hidden_size, output_size, num_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.hidden_channel_size = hidden_channel_size\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.conv1 = nn.Conv2d(self.in_channels, self.hidden_channel_size, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(self.hidden_channel_size, self.hidden_channel_size, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(self.hidden_channel_size, 1, kernel_size=3, padding=1)\n",
        "    self.gru = nn.GRU(input_size**2, hidden_size, num_layers)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input_img, hidden):\n",
        "    input_img = input_img.unsqueeze(0) # unsqueeze to comply with mini-batch for conv2d\n",
        "    out = self.conv1(input_img)\n",
        "    out = self.conv2(out)\n",
        "    out = self.conv3(out)\n",
        "    out = out.view(1, -1, self.input_size**2) # reshape to expected shape for gru\n",
        "    out, hidden = self.gru(out, hidden)\n",
        "    out = self.linear(out)\n",
        "\n",
        "    return out.view(-1), hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.num_layers, 1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YioztDVEZwPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "74a1c54d-f3c2-490a-8942-8efb0093b3ad"
      },
      "source": [
        "batch_size = 5\n",
        "input_size = 10\n",
        "num_layers = 2\n",
        "hidden_size = 20\n",
        "seq_len = 3\n",
        "rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
        "inp = torch.randn(batch_size, seq_len, input_size)\n",
        "h0 = torch.randn(num_layers, seq_len, hidden_size)\n",
        "output, hn = rnn(inp, h0)\n",
        "print(h0.shape)\n",
        "print(output.shape)\n",
        "print(output.view(-1, output.shape[1] * output.shape[2]).shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 20])\n",
            "torch.Size([5, 3, 20])\n",
            "torch.Size([5, 60])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "279b31db-b391-42d4-de21-f9b8350df8b4",
        "id": "4CnmylxIhRnG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# rnn = nn.GRU()\n",
        "\n",
        "conv = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "conv3 = nn.Conv2d(32, 10, kernel_size=3, padding=1)\n",
        "\n",
        "# dataset[0][0][0].unsqueeze(0).shape\n",
        "\n",
        "conv3(conv2(conv(dataset[0][0][0].unsqueeze(0)))).shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 512, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQVeW1aq18tB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "112c57bb-4ae8-4658-8061-1ff67d01ffdc"
      },
      "source": [
        "dataset = EsposallesDataset(img_size=512)\n",
        "\n",
        "\n",
        "x = dataset[0][0][0]\n",
        "model = RNN(3, 50, 512, 200, 6, num_layers=3)\n",
        "hidden0 = model.init_hidden()\n",
        "y_hat, hidden = model(x, hidden0)\n",
        "hidden.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz_0aaDcevwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "5db1e600-3a70-4787-b5c2-524b40ad39b0"
      },
      "source": [
        "input_size = 256\n",
        "\n",
        "dataset = EsposallesDataset(img_size=input_size)\n",
        "data_loader = DataLoader(train_dataset,\n",
        "                          batch_size=1,\n",
        "                          num_workers=4,\n",
        "                          shuffle=False)\n",
        "\n",
        "\n",
        "hidden_size = 200\n",
        "output_size = dataset.num_labels()\n",
        "num_layers = 3\n",
        "\n",
        "model = RNN(3, input_size, hidden_size, output_size, num_layers=3)\n",
        "model = model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loop = tqdm(total=len(data_loader), position=0, leave=True)\n",
        "\n",
        "  for batch, (x, y_truth) in enumerate(data_loader):\n",
        "    x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "    print(x)\n",
        "    print(y_truth)\n",
        "\n",
        "    loss = 0\n",
        "    hidden = model.init_hidden()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    accs = []\n",
        "    losses = []\n",
        "\n",
        "    for word, label in zip(x, y_truth):\n",
        "      optimizer.zero_grad()\n",
        "      pred, hidden = model(word, hidden))\n",
        "\n",
        "      acc = torch.eq(y_hat.argmax(), label)\n",
        "      ls = objective(pred, label)\n",
        "\n",
        "      accs.append(acc.item())\n",
        "      losses.append(ls.item())\n",
        "\n",
        "      loss += ls\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loop.set_description('Epoch:{}, Loss:{}, Acc:{}'.format(epoch, np.mean(losses), np.mean(accs)))\n",
        "    loop.update(1)\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-107-96144feee76b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    input_size = pass\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}