{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs601r-dl/blob/master/DL_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v2tFVnpD--T",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2: Hyper Zoo\n",
        "\n",
        "### Deliverable:\n",
        "For this lab, you will submit an ipython notebook via learningsuite. Your notebook will contain two parts, as described below.\n",
        "\n",
        "### Grading standards:\n",
        "Your notebook will be graded on the following:\n",
        "\n",
        "* 35% Part 1: Clearly displayed 10 bars (one for baseline, one for each tweak \n",
        "independently)\n",
        "* 5% Part 1: Small writeup of conclusions from independent tweaks\n",
        "* 25% Part 2: Clear explanation of your tweaking strategy\n",
        "* 25% Part 2: Actually run your tweaking strategy and show the results\n",
        "* 10% Tidy and legible figures, including labeled axes where appropriate\n",
        "* 10% Extra credit - Error bars on your figure in Part 1.\n",
        "\n",
        "### Description:\n",
        "The goal of this lab is to learn how to explore the combinatorial space of possible hyperparameter settings.\n",
        "\n",
        "Many deep learning papers present some sort of tweak on standard deep learning, and empirically illustrate that it improves performance (ideally across a wide variety of architectures and datasets). It quickly becomes hard to know: which, if any, of these tweaks are truly important - and how do they work when combined?\n",
        "\n",
        "For this lab, you will explore various tweaks to the basic classifier you coded in lab 1. There are two parts to the lab.\n",
        "\n",
        "### Part 1\n",
        "You must clearly show the individual effect of each tweak compared to the baseline. For this part, you should present a simple bar chart (or possibly two or more, depending on your layout), clearly labeled with the baseline performance, and then the performance of each tweak relative to baseline. You may plot absolute or relative performances; whichever is clearer.\n",
        "\n",
        "You must include a few sentences describing what you can conclude from evaluating all of these tweaks.\n",
        "\n",
        "Note: I am not requiring error bars for this lab, because they are computationally intensive. I have made them extra credit – although if we were doing this for real, they would be absolutely required!\n",
        "\n",
        "### Part 2\n",
        "You must think about how to find the best combination of tweaks. There is no right answer to this part; I want you to think carefully about how to search the space of possible combinations, and come up with a reasonable method for settling on a final combination of tweaks. I have tried to provide enough tweaks that it should be impossible to brute-force try all possible combinations (although that is certainly a valid strategy!).\n",
        "\n",
        "For this part, you must include in your notebook a simple writeup describing your strategy (just a paragraph or two), and then show the final performance of whatever combination you hit upon.\n",
        "\n",
        "Note that you will not be graded on absolute performance of any run; what is important is thinking clearly through which tweaks make a difference.\n",
        "\n",
        "### The Tweaks\n",
        "Your baseline classifier must be a “vanilla” classifier, with none of the features listed below. We will systematically add them in.\n",
        "\n",
        "You must test the following:\n",
        "\n",
        "* Activation functions: relu (baseline), leakyrelu, selu, elu, hardshrink\n",
        "* Batchnorm: off (baseline), on (use one batchnorm per residual block)\n",
        "* Label smoothing: off (baseline), on\n",
        "* Learning rate: constant (baseline), CLR\n",
        "* Regularization: off (baseline), dropout\n",
        "* Initialization: xavier/he (baseline), orthogonal\n",
        "\n",
        "So, for part one, your bar chart should have 10 different bars.\n",
        "\n",
        "Some of these tweaks require additional parameters. You should either leave them at their default values, or think of some reasonable way to set them.\n",
        "\n",
        "Note: pytorch does not (AFAIK) natively implement label smoothing. In the interests of focusing on hyperparameter searching, you may verbatim copy any internet code you like to help implement label smoothing.\n",
        "\n",
        "### Hints\n",
        "Activation functions and dropout can all be found in torch.nn\n",
        "\n",
        "Initialization functions can be found in torch.nn.init\n",
        "\n",
        "This lab should be pretty straightforward, with the right script – you should be able to iterate over tweaks and run your classifier in a tidy loop. Ideally, you'll code it up, let it run, and come back in a few hours to find the results!\n",
        "\n",
        "If you find yourself cutting-and-pasting, you might want to rethink your strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLQ6AzvJ5TC",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8O60XWDIm2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfIE8Y_uP85c",
        "colab_type": "text"
      },
      "source": [
        "## Model Implementation - 20 Layer Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td9yv6vwQjrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, batch_norm=True, activation=nn.ReLU, init_orthogonal=False, dropout=False):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.batch_norm = batch_norm\n",
        "    self.dropout = dropout\n",
        "    self.activation_function = activation\n",
        "    self.activation = activation()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "    torch.nn.init.orthogonal_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.drop = nn.Dropout(p=0.2)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    if init_orthogonal:\n",
        "      nn.init.orthogonal_(self.conv1.weight)\n",
        "      nn.init.orthogonal_(self.conv2.weight)\n",
        "\n",
        "    if in_channels != out_channels:\n",
        "      self.needs_shortcut = True\n",
        "      self.conv_short = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "      self.bn_short = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "      if init_orthogonal:\n",
        "        nn.init.orthogonal_(self.conv_short.weight)\n",
        "\n",
        "      if batch_norm:\n",
        "        self.shortcut = nn.Sequential(\n",
        "          self.conv_short,\n",
        "          self.bn_short\n",
        "        )\n",
        "      else:\n",
        "        self.shortcut = self.conv_short\n",
        "\n",
        "    else:\n",
        "      self.needs_shortcut = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x if not self.needs_shortcut else self.shortcut(x)\n",
        "    out = self.conv1(x)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      out = self.bn1(out)\n",
        "\n",
        "    out = self.activation(out)\n",
        "\n",
        "    if self.dropout:\n",
        "      out = self.drop(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      out = self.bn2(out)\n",
        "\n",
        "    out += identity\n",
        "    final_activation = self.activation_function()\n",
        "\n",
        "    return final_activation(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Igh7d8QAJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Resnet(nn.Module):\n",
        "  def __init__(self, dataset, num_classes, batch_norm=False, activation=nn.ReLU, init_orthogonal=False, dropout=False):\n",
        "    super(Resnet, self).__init__()\n",
        "\n",
        "    x, y = dataset[0]\n",
        "    in_channels, height, width = x.size()\n",
        "    cstart = 64\n",
        "\n",
        "    layers = []\n",
        "    layers.append(ResidualBlock(in_channels, cstart, batch_norm=batch_norm, activation=activation, init_orthogonal=init_orthogonal, dropout=dropout))\n",
        "    for i in range(19):\n",
        "      layers.append(ResidualBlock(cstart, cstart, batch_norm=batch_norm, activation=activation, init_orthogonal=init_orthogonal, dropout=dropout))\n",
        "  \n",
        "    self.net = nn.Sequential(*layers)\n",
        "    self.fc1 = nn.Linear(cstart * height * width, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Residual Blocks\n",
        "    out = self.net(x)\n",
        "\n",
        "    # Flatten and narrow down to the number of classes\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.fc1(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fotNuyJSBye",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7zWqzewaZql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.0):\n",
        "        super(LabelSmoothLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        log_prob = F.log_softmax(input, dim=-1)\n",
        "        weight = input.new_ones(input.size()) * \\\n",
        "            self.smoothing / (input.size(-1) - 1.)\n",
        "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
        "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UohDS3xBrrzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False, adam=False, lrbase=1e-4, lrmax=1e-2):\n",
        "  NUM_EPOCHS = 5\n",
        "  BATCH_SIZE = 300\n",
        "\n",
        "  train_dataset = torchvision.datasets.CIFAR10('/content/cifar-train', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "  val_dataset = torchvision.datasets.CIFAR10('/content/cifar-val', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "  model = Resnet(train_dataset, 10, batch_norm=batch_norm, activation=activation, init_orthogonal=init_orthogonal, dropout=dropout) #PytorchResnet(dataset.num_classes())\n",
        "  model = model.cuda()\n",
        "\n",
        "  if label_smoothing:\n",
        "    objective = LabelSmoothLoss(smoothing=0.00)\n",
        "  else:\n",
        "    objective = LabelSmoothLoss(smoothing=0.05)\n",
        "\n",
        "  if adam:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lrbase)\n",
        "  else:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lrbase)\n",
        "\n",
        "  if clr:\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lrbase, max_lr=lrmax)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            num_workers=4,\n",
        "                            shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True)\n",
        "  \n",
        "  accuracy = 0\n",
        "  \n",
        "  predictions = np.array([])\n",
        "  actual = np.array([])\n",
        "  \n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loop = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "\n",
        "    for batch, (x, y_truth) in enumerate(train_loader):\n",
        "      gc.collect()\n",
        "      x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x)\n",
        "\n",
        "      loss = objective(y_hat, y_truth.long())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      loop.set_description('Train - epoch:{}, loss:{:.4f}'.format(epoch, loss.item()))\n",
        "      loop.update(1)\n",
        "\n",
        "      optimizer.step()\n",
        "      if clr:\n",
        "        scheduler.step()\n",
        "\n",
        "    if epoch == (NUM_EPOCHS - 1):\n",
        "      with torch.no_grad():\n",
        "\n",
        "        val_single_acc = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for _, (val_x, val_y_truth) in enumerate(val_loader):\n",
        "          gc.collect()\n",
        "          val_x, val_y_truth = val_x.cuda(), val_y_truth.cuda()\n",
        "\n",
        "          val_y_hat = model(val_x)\n",
        "          val_acc = torch.eq(val_y_hat.argmax(1), val_y_truth.long()).float().mean()\n",
        "\n",
        "          val_single_acc.append(val_acc.item())\n",
        "\n",
        "        accuracy = np.mean(val_single_acc)\n",
        "        print('\\nValidation - epoch:{}, acc:{:.4f}'.format(epoch, accuracy))\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4F96Ge5Tm9R",
        "colab_type": "text"
      },
      "source": [
        "## Part I Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PF4NkHbTrHx",
        "colab_type": "code",
        "outputId": "52adf28a-6ca7-498e-fa95-4c327a72615a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Obtain accuracies for all tweaks:\n",
        "# Accuracies are the results given on the validation set after 5 epochs\n",
        "try:\n",
        "  baseline = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished baseline')\n",
        "  leakyrelu = train(activation=nn.LeakyReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished leakyrelu')\n",
        "  selu = train(activation=nn.SELU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished selu')\n",
        "  elu = train(activation=nn.ELU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished elu')\n",
        "  hardshrink = train(activation=nn.Hardshrink, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished hardshrink')\n",
        "  batchnorm = train(activation=nn.ReLU, batch_norm=True, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished batchnorm')\n",
        "  labelsmoothing = train(activation=nn.ReLU, batch_norm=False, label_smoothing=True, clr=False, dropout=False, init_orthogonal=False)\n",
        "  print('Finished labelsmoothing')\n",
        "  clr = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=True, dropout=False, init_orthogonal=False)\n",
        "  print('Finished clr')\n",
        "  dropout = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=True, init_orthogonal=False)\n",
        "  print('Finished dropout')\n",
        "  orthogonal = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=True)\n",
        "  print('Finished orthogonal')\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.2121: 100%|██████████| 167/167 [03:48<00:00,  1.24s/it]\n",
            "Train - epoch:1, loss:2.1154: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]\n",
            "Train - epoch:2, loss:2.0858: 100%|██████████| 167/167 [03:48<00:00,  1.24s/it]\n",
            "Train - epoch:3, loss:1.9915: 100%|██████████| 167/167 [03:49<00:00,  1.23s/it]\n",
            "Train - epoch:4, loss:1.9524: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.3319\n",
            "Finished baseline\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.1510: 100%|██████████| 167/167 [03:50<00:00,  1.25s/it]\n",
            "Train - epoch:1, loss:2.0888: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:2, loss:2.0741: 100%|██████████| 167/167 [03:50<00:00,  1.24s/it]\n",
            "Train - epoch:3, loss:1.9309: 100%|██████████| 167/167 [03:49<00:00,  1.23s/it]\n",
            "Train - epoch:4, loss:1.9672: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.3483\n",
            "Finished leakyrelu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:1.8351: 100%|██████████| 167/167 [03:48<00:00,  1.24s/it]\n",
            "Train - epoch:1, loss:1.6914: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]\n",
            "Train - epoch:2, loss:1.5831: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:3, loss:1.5596: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:4, loss:1.5345: 100%|██████████| 167/167 [03:50<00:00,  1.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.4788\n",
            "Finished selu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.0329: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:1, loss:1.9245: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:2, loss:1.9121: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:3, loss:1.7246: 100%|██████████| 167/167 [03:49<00:00,  1.23s/it]\n",
            "Train - epoch:4, loss:1.8165: 100%|██████████| 167/167 [03:49<00:00,  1.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.4100\n",
            "Finished elu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.2835: 100%|██████████| 167/167 [03:44<00:00,  1.21s/it]\n",
            "Train - epoch:1, loss:2.2328: 100%|██████████| 167/167 [03:43<00:00,  1.21s/it]\n",
            "Train - epoch:2, loss:2.1826: 100%|██████████| 167/167 [03:43<00:00,  1.21s/it]\n",
            "Train - epoch:3, loss:2.1088: 100%|██████████| 167/167 [03:43<00:00,  1.21s/it]\n",
            "Train - epoch:4, loss:2.0987: 100%|██████████| 167/167 [03:43<00:00,  1.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.2719\n",
            "Finished hardshrink\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:6.2869: 100%|██████████| 167/167 [04:07<00:00,  1.34s/it]\n",
            "Train - epoch:1, loss:4.2877: 100%|██████████| 167/167 [04:07<00:00,  1.34s/it]\n",
            "Train - epoch:2, loss:2.7639: 100%|██████████| 167/167 [04:07<00:00,  1.34s/it]\n",
            "Train - epoch:3, loss:2.6057: 100%|██████████| 167/167 [04:07<00:00,  1.34s/it]\n",
            "Train - epoch:4, loss:2.4060: 100%|██████████| 167/167 [04:07<00:00,  1.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.4003\n",
            "Finished batchnorm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.1539: 100%|██████████| 167/167 [03:47<00:00,  1.23s/it]\n",
            "Train - epoch:1, loss:2.0614: 100%|██████████| 167/167 [03:47<00:00,  1.24s/it]\n",
            "Train - epoch:2, loss:1.9933: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]\n",
            "Train - epoch:3, loss:1.9631: 100%|██████████| 167/167 [03:47<00:00,  1.23s/it]\n",
            "Train - epoch:4, loss:1.9065: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.3269\n",
            "Finished labelsmoothing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:1.7753: 100%|██████████| 167/167 [03:48<00:00,  1.23s/it]\n",
            "Train - epoch:1, loss:1.5918: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:2, loss:1.4409: 100%|██████████| 167/167 [03:49<00:00,  1.23s/it]\n",
            "Train - epoch:3, loss:1.3456: 100%|██████████| 167/167 [03:49<00:00,  1.24s/it]\n",
            "Train - epoch:4, loss:1.1589: 100%|██████████| 167/167 [03:50<00:00,  1.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.6324\n",
            "Finished clr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.1461: 100%|██████████| 167/167 [03:53<00:00,  1.26s/it]\n",
            "Train - epoch:1, loss:2.1748: 100%|██████████| 167/167 [03:52<00:00,  1.25s/it]\n",
            "Train - epoch:2, loss:2.1688: 100%|██████████| 167/167 [03:52<00:00,  1.26s/it]\n",
            "Train - epoch:3, loss:2.0728: 100%|██████████| 167/167 [03:52<00:00,  1.26s/it]\n",
            "Train - epoch:4, loss:2.0896: 100%|██████████| 167/167 [03:52<00:00,  1.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.3141\n",
            "Finished dropout\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.0556: 100%|██████████| 167/167 [03:57<00:00,  1.32s/it]\n",
            "Train - epoch:1, loss:1.8839: 100%|██████████| 167/167 [04:05<00:00,  1.34s/it]\n",
            "Train - epoch:2, loss:1.8697: 100%|██████████| 167/167 [04:05<00:00,  1.33s/it]\n",
            "Train - epoch:3, loss:1.8845: 100%|██████████| 167/167 [04:05<00:00,  1.34s/it]\n",
            "Train - epoch:4, loss:1.7577: 100%|██████████| 167/167 [04:06<00:00,  1.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.4150\n",
            "Finished orthogonal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n0yaIqKWXd-J",
        "colab": {}
      },
      "source": [
        "# Accuracies\n",
        "baseline = .3319\n",
        "leakyrelu = .3483\n",
        "selu = .4788\n",
        "elu = .4100\n",
        "hardshrink = .2719\n",
        "batchnorm = .4003\n",
        "labelsmoothing = .3269\n",
        "clr = .6324\n",
        "dropout = .3141\n",
        "orthogonal = .4150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86NoGNDhRSG",
        "colab_type": "code",
        "outputId": "79dd644c-1a0f-420e-cdec-7e23a78364c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(12, 9))\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "labels = ['Baseline', 'LeakyReLU', 'SELU', 'ELU', 'HardShrink', 'BatchNorm', 'LabelSmoothing', 'CLR', 'Dropout', 'Orthogonal']\n",
        "accuracies = [baseline, leakyrelu, selu, elu, hardshrink, batchnorm, labelsmoothing, clr, dropout, orthogonal]\n",
        "ax.bar(labels, accuracies)\n",
        "plt.axhline(y=baseline,linewidth=1, color='k')\n",
        "plt.title(\"Accuracy By Individual Tweak\")\n",
        "plt.xlabel('Tweak')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5MAAALFCAYAAABeT7fXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDqtRFiNX_uA",
        "colab_type": "text"
      },
      "source": [
        "### Part I Conclusion\n",
        "\n",
        "Looking at the individual tweaks, SELU outperformed all other activation functions. I was surprised that Orthogonal initialization had such a positive effect individually as well. CLR performed remarkably well compared to the other tweaks given. I assume this is because SGD was used over Adam. Adam may give more comparable results.\n",
        "\n",
        "Overall, I was surprised that ReLU was second to worst on the activation functions we tried. This leads me to conclude that I should look at the activation function more in the future. Batch_Norm seems to always help. Dropout seems to only help if the network obtains high accuracy on the training data and is overfitting. I also want to look more in the future at initialization strategies, as orthogonal performed better than I anticipated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTdsEGTitpI4",
        "colab_type": "text"
      },
      "source": [
        "## Part II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8xETMe8xtLs",
        "colab_type": "text"
      },
      "source": [
        "### Combination 1\n",
        "\n",
        "Idea:\n",
        "* Take all of the tweaks that performed above the baseline (using SELU, the best performing activation function)\n",
        "\n",
        "Results:\n",
        "* Loss was crazy at first, and still had a hard time settling down to some extent. I assume that it has something to do with the combination of SELU and CLR. It also could have something to do with the self-normalizing concept of SELU and an incorrect initialization strategy in conjunction with using batch_norm.\n",
        "* The SNN paper described a Gaussian initialization strategy and a special kind of dropout due to its self-normalizing behavior. Let's remove SELU since we aren't following the given procedure.\n",
        "* Overall, the network performed quite well comparitively on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgWipUBhtsZv",
        "colab_type": "code",
        "outputId": "9eb12ce1-7ef2-44fb-f012-bbc608f439f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "try:\n",
        "  cbn1 = train(activation=nn.SELU, batch_norm=True, label_smoothing=False, clr=True, dropout=False, init_orthogonal=True)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.4243: 100%|██████████| 167/167 [04:28<00:00,  1.45s/it]\n",
            "Train - epoch:1, loss:2.0539: 100%|██████████| 167/167 [04:27<00:00,  1.46s/it]\n",
            "Train - epoch:2, loss:1.5533: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]\n",
            "Train - epoch:3, loss:1.2143: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]\n",
            "Train - epoch:4, loss:1.0306: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.5197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3d-O391yeTh",
        "colab_type": "text"
      },
      "source": [
        "### Combination 2\n",
        "\n",
        "Idea:\n",
        "* Change from SELU to the next best performing activation function due to the assumption that SELU and CLR was causing problems.\n",
        "\n",
        "Results:\n",
        "* Whether it was batch_norm or CLR, something was messing with SELU to get worse results.\n",
        "* This shows that switching the activation function to ELU helped, likely due to the self-normalizing behavior of SELU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8E-LHy12-t_",
        "colab_type": "code",
        "outputId": "e1a61e98-3913-43e2-cd8b-23d1880945b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "try:\n",
        "  cbn2 = train(activation=nn.ELU, batch_norm=True, label_smoothing=False, clr=True, dropout=False, init_orthogonal=True)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:1.7373: 100%|██████████| 167/167 [04:25<00:00,  1.45s/it]\n",
            "Train - epoch:1, loss:1.2451: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]\n",
            "Train - epoch:2, loss:0.9868: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]\n",
            "Train - epoch:3, loss:0.9708: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]\n",
            "Train - epoch:4, loss:0.9062: 100%|██████████| 167/167 [04:27<00:00,  1.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.6139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56tng1vQ6Mmh",
        "colab_type": "text"
      },
      "source": [
        "### Combination 3\n",
        "\n",
        "Idea:\n",
        "* I noticed that the loss for the previous two combinations was lower than the original CLR run. However, the accuracy wasn't nearly as good. Maybe the network is overfitting on the training set just slighly. Let's add in dropout.\n",
        "\n",
        "Results:\n",
        "* It seems that adding dropout helped increase the accuracy by 4 percentage points.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLUHkR456N6g",
        "colab_type": "code",
        "outputId": "fbb0bc9b-ac63-4ca2-cb40-c6f236fcd5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "try:\n",
        "  cbn3 = train(activation=nn.ELU, batch_norm=True, label_smoothing=False, clr=True, dropout=True, init_orthogonal=True)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.0086: 100%|██████████| 167/167 [04:26<00:00,  1.47s/it]\n",
            "Train - epoch:1, loss:1.6540: 100%|██████████| 167/167 [04:32<00:00,  1.49s/it]\n",
            "Train - epoch:2, loss:1.5107: 100%|██████████| 167/167 [04:31<00:00,  1.48s/it]\n",
            "Train - epoch:3, loss:1.2175: 100%|██████████| 167/167 [04:31<00:00,  1.47s/it]\n",
            "Train - epoch:4, loss:1.0754: 100%|██████████| 167/167 [04:31<00:00,  1.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.6538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZxsUyGC63Bx",
        "colab_type": "text"
      },
      "source": [
        "### Combination 4\n",
        "Idea:\n",
        "* Thus far, we have been using SGD as the optimizer (to allow CLR to work natively in pytorch). Let's keep all the tweaks that helped improve accuracy, but remove CLR in favor of a standalone Adam optimizer.\n",
        "\n",
        "Results:\n",
        "* The higher base learning rate and max learning rate resulted in massive losses at the beginning until we got the gradients settled down.\n",
        "* Although it improved, the losses were still jumping all over the place. Adjusting the learning rate is likely a huge factor in determing the success of your model (We only have a finite amount of time to train)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nszb51n7Ou4",
        "colab_type": "code",
        "outputId": "f2d67f71-31fe-4324-e3d0-42004ec4a745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "try:\n",
        "  cbn4 = train(activation=nn.ELU, batch_norm=True, label_smoothing=False, clr=False, dropout=True, init_orthogonal=True, adam=True)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:1.8142: 100%|██████████| 167/167 [04:35<00:00,  1.50s/it]\n",
            "Train - epoch:1, loss:1.6508: 100%|██████████| 167/167 [04:34<00:00,  1.49s/it]\n",
            "Train - epoch:2, loss:1.4429: 100%|██████████| 167/167 [04:35<00:00,  1.49s/it]\n",
            "Train - epoch:3, loss:1.2944: 100%|██████████| 167/167 [04:35<00:00,  1.50s/it]\n",
            "Train - epoch:4, loss:1.1345: 100%|██████████| 167/167 [04:34<00:00,  1.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.6146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKt-RFBGFdFT",
        "colab_type": "text"
      },
      "source": [
        "### Combination 5\n",
        "Idea:\n",
        "* Increase the learning rate for Adam and use a different initialization. Maybe we simply need more time to converge or a quicker learning rate.\n",
        "\n",
        "Results:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7r-Y-d9Hhp0",
        "colab_type": "code",
        "outputId": "734ee011-f0bb-424c-eec2-614b4d5ca810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "try:\n",
        "  cbn5 = train(activation=nn.ELU, batch_norm=True, label_smoothing=False, clr=False, dropout=True, init_orthogonal=False, adam=True, lrbase=1e-3, lrmax=5e-2)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.3053: 100%|██████████| 167/167 [04:35<00:00,  1.49s/it]\n",
            "Train - epoch:1, loss:1.3723: 100%|██████████| 167/167 [04:34<00:00,  1.50s/it]\n",
            "Train - epoch:2, loss:1.5377: 100%|██████████| 167/167 [04:35<00:00,  1.50s/it]\n",
            "Train - epoch:3, loss:1.2626: 100%|██████████| 167/167 [04:34<00:00,  1.50s/it]\n",
            "Train - epoch:4, loss:1.1360: 100%|██████████| 167/167 [04:34<00:00,  1.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.6608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmzT_JqfNrPg",
        "colab_type": "text"
      },
      "source": [
        "### Combination 6\n",
        "\n",
        "Idea:\n",
        "* Maybe the learning rate using SGD was a little too low and it simply couldn't converge quick enough. What if we increase the bounds of the learning rate slightly to see if we converge quicker, while still using CLR.\n",
        "\n",
        "Results:\n",
        "* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnxTqVF6OGxA",
        "colab_type": "code",
        "outputId": "27196679-014a-49bc-d70f-489dbf98cdca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "try:\n",
        "  cbn6 = train(activation=nn.ELU, batch_norm=True, label_smoothing=False, clr=True, dropout=True, init_orthogonal=True, adam=False, lrbase=1e-3, lrmax=5e-2)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.2606: 100%|██████████| 167/167 [04:31<00:00,  1.47s/it]\n",
            "Train - epoch:1, loss:1.8941: 100%|██████████| 167/167 [04:31<00:00,  1.47s/it]\n",
            "Train - epoch:2, loss:1.5508: 100%|██████████| 167/167 [04:31<00:00,  1.47s/it]\n",
            "Train - epoch:3, loss:1.3968: 100%|██████████| 167/167 [04:31<00:00,  1.48s/it]\n",
            "Train - epoch:4, loss:1.3752: 100%|██████████| 167/167 [04:31<00:00,  1.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:4, acc:0.5264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDRo_8DsLoCr",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "When searching for the best hyper-parameters, I think a great place to start is by choosing your activation function. The activation may determine the impact on the rest of the hyper parameters that you choose to select. For example, SELU performs best when weights are initialized with Gaussian and a special kind of dropout is used (to preserve self-normalization). Thus, the hyper-parameters of batch norm, initialization, and regularization will be greatly dependent on your choice of activation function.\n",
        "\n",
        "Cyclic learning rates improve performance. However, they tend not to work all that well with adaptive optimizers. It seems that CLR should be used whenever you use SGD or anything non-adaptive. There is a trade-off here, because usually an optimizer like Adam will converge much quicker than SGD. This may require some tuning to discover which works better. Along these lines, the chosen learning rates are extremely important as well.\n",
        "\n",
        "Overall, my best set of hyper-parameters included ELU, Batch_Norm, Label_Smoothing, Adam Optimizer, Dropout, and Xavier initialization.\n",
        "Compared to the baseline performance, the best hyper-parameter combination jumped over 30 percentage points. There was also nearly a 50 percentage point difference between the best hyper-parameter combination and the worst hyper-parameter combination. Are hyper-parameters important? I think so!"
      ]
    }
  ]
}