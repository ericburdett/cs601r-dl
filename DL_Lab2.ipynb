{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs601r-dl/blob/master/DL_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v2tFVnpD--T",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2: Hyper Zoo\n",
        "\n",
        "### Deliverable:\n",
        "For this lab, you will submit an ipython notebook via learningsuite. Your notebook will contain two parts, as described below.\n",
        "\n",
        "### Grading standards:\n",
        "Your notebook will be graded on the following:\n",
        "\n",
        "* 35% Part 1: Clearly displayed 10 bars (one for baseline, one for each tweak \n",
        "independently)\n",
        "* 5% Part 1: Small writeup of conclusions from independent tweaks\n",
        "* 25% Part 2: Clear explanation of your tweaking strategy\n",
        "* 25% Part 2: Actually run your tweaking strategy and show the results\n",
        "* 10% Tidy and legible figures, including labeled axes where appropriate\n",
        "* 10% Extra credit - Error bars on your figure in Part 1.\n",
        "\n",
        "### Description:\n",
        "The goal of this lab is to learn how to explore the combinatorial space of possible hyperparameter settings.\n",
        "\n",
        "Many deep learning papers present some sort of tweak on standard deep learning, and empirically illustrate that it improves performance (ideally across a wide variety of architectures and datasets). It quickly becomes hard to know: which, if any, of these tweaks are truly important - and how do they work when combined?\n",
        "\n",
        "For this lab, you will explore various tweaks to the basic classifier you coded in lab 1. There are two parts to the lab.\n",
        "\n",
        "### Part 1\n",
        "You must clearly show the individual effect of each tweak compared to the baseline. For this part, you should present a simple bar chart (or possibly two or more, depending on your layout), clearly labeled with the baseline performance, and then the performance of each tweak relative to baseline. You may plot absolute or relative performances; whichever is clearer.\n",
        "\n",
        "You must include a few sentences describing what you can conclude from evaluating all of these tweaks.\n",
        "\n",
        "Note: I am not requiring error bars for this lab, because they are computationally intensive. I have made them extra credit – although if we were doing this for real, they would be absolutely required!\n",
        "\n",
        "### Part 2\n",
        "You must think about how to find the best combination of tweaks. There is no right answer to this part; I want you to think carefully about how to search the space of possible combinations, and come up with a reasonable method for settling on a final combination of tweaks. I have tried to provide enough tweaks that it should be impossible to brute-force try all possible combinations (although that is certainly a valid strategy!).\n",
        "\n",
        "For this part, you must include in your notebook a simple writeup describing your strategy (just a paragraph or two), and then show the final performance of whatever combination you hit upon.\n",
        "\n",
        "Note that you will not be graded on absolute performance of any run; what is important is thinking clearly through which tweaks make a difference.\n",
        "\n",
        "### The Tweaks\n",
        "Your baseline classifier must be a “vanilla” classifier, with none of the features listed below. We will systematically add them in.\n",
        "\n",
        "You must test the following:\n",
        "\n",
        "* Activation functions: relu (baseline), leakyrelu, selu, elu, hardshrink\n",
        "* Batchnorm: off (baseline), on (use one batchnorm per residual block)\n",
        "* Label smoothing: off (baseline), on\n",
        "* Learning rate: constant (baseline), CLR\n",
        "* Regularization: off (baseline), dropout\n",
        "* Initialization: xavier/he (baseline), orthogonal\n",
        "\n",
        "So, for part one, your bar chart should have 10 different bars.\n",
        "\n",
        "Some of these tweaks require additional parameters. You should either leave them at their default values, or think of some reasonable way to set them.\n",
        "\n",
        "Note: pytorch does not (AFAIK) natively implement label smoothing. In the interests of focusing on hyperparameter searching, you may verbatim copy any internet code you like to help implement label smoothing.\n",
        "\n",
        "### Hints\n",
        "Activation functions and dropout can all be found in torch.nn\n",
        "\n",
        "Initialization functions can be found in torch.nn.init\n",
        "\n",
        "This lab should be pretty straightforward, with the right script – you should be able to iterate over tweaks and run your classifier in a tidy loop. Ideally, you'll code it up, let it run, and come back in a few hours to find the results!\n",
        "\n",
        "If you find yourself cutting-and-pasting, you might want to rethink your strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLQ6AzvJ5TC",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8O60XWDIm2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfIE8Y_uP85c",
        "colab_type": "text"
      },
      "source": [
        "## Model Implementation - 20 Layer Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt0RBBAj4N_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For testing purposes - To get a baseline for performance\n",
        "class PytorchResnet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(PytorchResnet, self).__init__()\n",
        "\n",
        "    self.model = torchvision.models.resnet152(pretrained=True)\n",
        "\n",
        "    fc = nn.Linear(in_features=2048, out_features=num_classes)\n",
        "    fc.requires_grad=True\n",
        "    self.model.fc = fc\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td9yv6vwQjrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, batch_norm=True, activation=nn.ReLU, init_orthogonal=False, dropout=False):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.batch_norm = batch_norm\n",
        "    self.dropout = dropout\n",
        "    self.activation_function = activation\n",
        "    self.activation = activation()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "    torch.nn.init.orthogonal_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.drop = nn.Dropout(p=0.2)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    if init_orthogonal:\n",
        "      torch.init.orthogonol_(self.conv1.weight)\n",
        "      torch.init.orthogonol_(self.conv2.weight)\n",
        "\n",
        "    if in_channels != out_channels:\n",
        "      self.needs_shortcut = True\n",
        "      self.conv_short = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "      self.bn_short = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "      if init_orthogonal:\n",
        "        torch.init.orthogonal_(self.conv_short.weight)\n",
        "\n",
        "      if batch_norm:\n",
        "        self.shortcut = nn.Sequential(\n",
        "          self.conv_short,\n",
        "          self.bn_short\n",
        "        )\n",
        "      else:\n",
        "        self.shortcut = self.conv_short\n",
        "\n",
        "    else:\n",
        "      self.needs_shortcut = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x if not self.needs_shortcut else self.shortcut(x)\n",
        "    out = self.conv1(x)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      out = self.bn1(out)\n",
        "\n",
        "    out = self.activation(out)\n",
        "\n",
        "    if self.dropout:\n",
        "      out = self.drop(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      out = self.bn2(out)\n",
        "\n",
        "    out += identity\n",
        "    final_activation = self.activation_function()\n",
        "\n",
        "    return final_activation(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Igh7d8QAJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Resnet(nn.Module):\n",
        "  def __init__(self, dataset, num_classes, batch_norm=False, activation=nn.ReLU, init_orthogonal=False, dropout=False):\n",
        "    super(Resnet, self).__init__()\n",
        "\n",
        "    x, y = dataset[0]\n",
        "    in_channels, height, width = x.size()\n",
        "    cstart = 64\n",
        "\n",
        "    layers = []\n",
        "    layers.append(ResidualBlock(in_channels, cstart, batch_norm=batch_norm, activation=activation, init_orthogonal=init_orthogonal, dropout=dropout))\n",
        "    for i in range(19):\n",
        "      layers.append(ResidualBlock(cstart, cstart, batch_norm=batch_norm, activation=activation, init_orthogonal=init_orthogonal, dropout=dropout))\n",
        "  \n",
        "    self.net = nn.Sequential(*layers)\n",
        "    self.fc1 = nn.Linear(cstart * height * width, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Residual Blocks\n",
        "    out = self.net(x)\n",
        "\n",
        "    # Flatten and narrow down to the number of classes\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.fc1(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fotNuyJSBye",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7zWqzewaZql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.0):\n",
        "        super(LabelSmoothLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        log_prob = F.log_softmax(input, dim=-1)\n",
        "        weight = input.new_ones(input.size()) * \\\n",
        "            self.smoothing / (input.size(-1) - 1.)\n",
        "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
        "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UohDS3xBrrzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False):\n",
        "  NUM_EPOCHS = 1\n",
        "  BATCH_SIZE = 300\n",
        "\n",
        "  train_dataset = torchvision.datasets.CIFAR10('/content/cifar-train', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "  val_dataset = torchvision.datasets.CIFAR10('/content/cifar-val', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "  model = Resnet(train_dataset, 10, batch_norm=batch_norm, activation=activation, init_orthogonal=init_orthogonal, dropout=dropout) #PytorchResnet(dataset.num_classes())\n",
        "  model = model.cuda()\n",
        "\n",
        "  if label_smoothing:\n",
        "    objective = LabelSmoothLoss(smoothing=0.00)\n",
        "  else:\n",
        "    objective = LabelSmoothLoss(smoothing=0.05)\n",
        "\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
        "\n",
        "  if clr:\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-2)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            num_workers=4,\n",
        "                            shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True)\n",
        "  \n",
        "  accuracy = 0\n",
        "  \n",
        "  predictions = np.array([])\n",
        "  actual = np.array([])\n",
        "  \n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loop = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "\n",
        "    for batch, (x, y_truth) in enumerate(train_loader):\n",
        "      gc.collect()\n",
        "      x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x)\n",
        "\n",
        "      loss = objective(y_hat, y_truth.long())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      loop.set_description('Train - epoch:{}, loss:{:.4f}'.format(epoch, loss.item()))\n",
        "      loop.update(1)\n",
        "\n",
        "      optimizer.step()\n",
        "      if clr:\n",
        "        scheduler.step()\n",
        "\n",
        "    if epoch == (NUM_EPOCHS - 1):\n",
        "      with torch.no_grad():\n",
        "\n",
        "        val_single_acc = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for _, (val_x, val_y_truth) in enumerate(val_loader):\n",
        "          gc.collect()\n",
        "          val_x, val_y_truth = val_x.cuda(), val_y_truth.cuda()\n",
        "\n",
        "          val_y_hat = model(val_x)\n",
        "          val_acc = torch.eq(val_y_hat.argmax(1), val_y_truth.long()).float().mean()\n",
        "\n",
        "          val_single_acc.append(val_acc.item())\n",
        "\n",
        "        accuracy = np.mean(val_single_acc)\n",
        "        print('\\nValidation - epoch:{}, acc:{:.4f}'.format(epoch, accuracy))\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4F96Ge5Tm9R",
        "colab_type": "text"
      },
      "source": [
        "## Part I Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IehhbyPia1UW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "079ee00a-261d-4a0c-ac1a-885ff172a7fe"
      },
      "source": [
        "try:\n",
        "  baseline = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "except:\n",
        "  __ITB__()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train - epoch:0, loss:2.3025: 100%|██████████| 167/167 [03:43<00:00,  1.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation - epoch:0, acc:0.1004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PF4NkHbTrHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtain accuracies for all tweaks:\n",
        "baseline = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "leakyrelu = train(activation=nn.LeakyReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "selu = train(activation=nn.SELU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "elu = train(activation=nn.ELU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "hardshrink = train(activation=nn.Hardshrink, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "batchnorm = train(activation=nn.ReLU, batch_norm=True, label_smoothing=False, clr=False, dropout=False, init_orthogonal=False)\n",
        "labelsmoothing = train(activation=nn.ReLU, batch_norm=False, label_smoothing=True, clr=False, dropout=False, init_orthogonal=False)\n",
        "clr = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=True, dropout=False, init_orthogonal=False)\n",
        "dropout = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=True, init_orthogonal=False)\n",
        "orthogonal = train(activation=nn.ReLU, batch_norm=False, label_smoothing=False, clr=False, dropout=False, init_orthogonal=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86NoGNDhRSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "labels = ['Baseline', 'LeakyReLU', 'SELU', 'ELU', 'HardShrink', 'BatchNorm', 'LabelSmoothing', 'CLR', 'Dropout', 'Orthogonal']\n",
        "accuracies = [baseline, leakyrelu, selu, elu, hardshrink, batchnorm, labelsmoothing, clr, dropout, orthogonal]\n",
        "ax.bar(labels, accuracies)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}